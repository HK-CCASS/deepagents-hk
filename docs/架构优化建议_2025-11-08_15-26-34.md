# HKEX é¡¹ç›®æ¶æ„ä¼˜åŒ–å»ºè®®æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: 2025-11-08_15-26-34
**åŸºäºæŠ¥å‘Š**: æ¶æ„åˆ†ææŠ¥å‘Š_2025-11-08_15-22-46.md
**å»ºè®®ç±»å‹**: å®æ–½å¯¼å‘çš„ä¼˜åŒ–æ–¹æ¡ˆ

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

åŸºäºå¯¹ HKEX é¡¹ç›®æ¶æ„çš„æ·±åº¦åˆ†æï¼Œæœ¬æŠ¥å‘Šæä¾›äº†**8ä¸ªå…³é”®ä¼˜åŒ–å»ºè®®**ï¼Œåˆ†ä¸ºé«˜ã€ä¸­ã€ä½ä¸‰ä¸ªä¼˜å…ˆçº§ï¼Œæ—¨åœ¨æå‡ç³»ç»Ÿçš„å¯é æ€§ã€æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§ã€‚

---

## ğŸš¨ **é«˜ä¼˜å…ˆçº§ä¼˜åŒ–** (ç«‹å³æ‰§è¡Œ)

### 1. **é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶**

#### ğŸ¯ **ç›®æ ‡**: æå‡ç³»ç»Ÿç¨³å®šæ€§å’Œå®¹é”™èƒ½åŠ›

#### ğŸ”§ **å®æ–½æ–¹æ¡ˆ**

**æ–‡ä»¶**: `src/services/hkex_api.py`

```python
# æ·»åŠ é‡è¯•è£…é¥°å™¨å’Œé”™è¯¯å¤„ç†
import asyncio
import structlog
from tenacity import retry, stop_after_attempt, wait_exponential
from typing import Optional, Dict, Any

logger = structlog.get_logger()

class EnhancedHKEXAPIService:
    """å¢å¼ºç‰ˆ HKEX API æœåŠ¡"""

    def __init__(self, timeout: int = 30, max_retries: int = 3):
        self.timeout = timeout
        self.max_retries = max_retries
        self.http_client = httpx.AsyncClient(
            timeout=timeout,
            limits=httpx.Limits(max_connections=10, max_keepalive_connections=5)
        )

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        reraise=True
    )
    async def fetch_with_retry(self, url: str, params: Dict[str, Any]) -> Dict:
        """å¸¦é‡è¯•æœºåˆ¶çš„å¼‚æ­¥ HTTP è¯·æ±‚"""
        try:
            logger.info("APIè¯·æ±‚å¼€å§‹", url=url, params=params)
            response = await self.http_client.get(
                url,
                params=params,
                headers=self.DEFAULT_HEADERS
            )
            response.raise_for_status()

            data = response.json()
            logger.info("APIè¯·æ±‚æˆåŠŸ", url=url, data_size=len(str(data)))
            return data

        except httpx.HTTPStatusError as e:
            logger.error(
                "HTTPé”™è¯¯",
                status_code=e.response.status_code,
                url=url,
                response_text=e.response.text
            )
            raise HKEXAPIError(f"HTTP {e.response.status_code}: {e.response.text}")

        except httpx.RequestError as e:
            logger.error("ç½‘ç»œè¯·æ±‚å¤±è´¥", url=url, error=str(e))
            raise HKEXAPIError(f"ç½‘ç»œé”™è¯¯: {str(e)}")

        except Exception as e:
            logger.error("æœªçŸ¥é”™è¯¯", url=url, error=str(e))
            raise HKEXAPIError(f"æœªçŸ¥é”™è¯¯: {str(e)}")

class HKEXAPIError(Exception):
    """HKEX API è‡ªå®šä¹‰å¼‚å¸¸"""
    pass
```

**é¢„æœŸæ•ˆæœ**:
- âœ… ç½‘ç»œé”™è¯¯è‡ªåŠ¨é‡è¯•ï¼Œæå‡æˆåŠŸç‡
- âœ… è¯¦ç»†é”™è¯¯æ—¥å¿—ï¼Œä¾¿äºé—®é¢˜æ’æŸ¥
- âœ… å¼‚æ­¥å¤„ç†æå‡å¹¶å‘æ€§èƒ½

#### ğŸ“Š **å®æ–½æ—¶é—´**: 2-3å¤©
#### ğŸ§ª **æµ‹è¯•ç­–ç•¥**: å•å…ƒæµ‹è¯• + é›†æˆæµ‹è¯•
#### ğŸ“ˆ **ROI**: é«˜ - æ˜¾è‘—æå‡ç³»ç»Ÿç¨³å®šæ€§

---

### 2. **ç»“æ„åŒ–æ—¥å¿—ä¸ç›‘æ§**

#### ğŸ¯ **ç›®æ ‡**: æå‡å¯è§‚æµ‹æ€§å’Œé—®é¢˜å®šä½èƒ½åŠ›

#### ğŸ”§ **å®æ–½æ–¹æ¡ˆ**

**æ–‡ä»¶**: `src/utils/logging.py` (æ–°å»º)

```python
import structlog
import sys
from typing import Any, Dict
from datetime import datetime
from contextlib import asynccontextmanager

class HKEXLogger:
    """HKEX ä¸“ç”¨æ—¥å¿—ç®¡ç†å™¨"""

    @staticmethod
    def configure_logging(log_level: str = "INFO"):
        """é…ç½®ç»“æ„åŒ–æ—¥å¿—"""
        structlog.configure(
            processors=[
                structlog.stdlib.filter_by_level,
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.stdlib.PositionalArgumentsFormatter(),
                structlog.processors.TimeStamper(fmt="iso"),
                structlog.processors.StackInfoRenderer(),
                structlog.processors.format_exc_info,
                structlog.processors.UnicodeDecoder(),
                structlog.processors.JSONRenderer()
            ],
            context_class=dict,
            logger_factory=structlog.stdlib.LoggerFactory(),
            wrapper_class=structlog.stdlib.BoundLogger,
            cache_logger_on_first_use=True,
        )

        logging.basicConfig(
            format="%(message)s",
            stream=sys.stdout,
            level=getattr(logging, log_level.upper())
        )

@asynccontextmanager
async def performance_monitor(operation_name: str, **context):
    """æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    start_time = datetime.now()
    logger = structlog.get_logger()

    logger.info("æ“ä½œå¼€å§‹", operation=operation_name, **context)

    try:
        yield
        duration = (datetime.now() - start_time).total_seconds()
        logger.info(
            "æ“ä½œå®Œæˆ",
            operation=operation_name,
            duration_seconds=duration,
            **context
        )
    except Exception as e:
        duration = (datetime.now() - start_time).total_seconds()
        logger.error(
            "æ“ä½œå¤±è´¥",
            operation=operation_name,
            duration_seconds=duration,
            error=str(e),
            error_type=type(e).__name__,
            **context
        )
        raise

# ä½¿ç”¨ç¤ºä¾‹
async def analyze_pdf(pdf_path: str) -> Dict:
    async with performance_monitor("PDFåˆ†æ", pdf_path=pdf_path):
        # PDF åˆ†æé€»è¾‘
        pass
```

**æ–‡ä»¶**: `src/monitoring/metrics.py` (æ–°å»º)

```python
from dataclasses import dataclass
from typing import Dict, Optional
from datetime import datetime, timedelta
import asyncio

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    operation: str
    start_time: datetime
    end_time: Optional[datetime] = None
    success: bool = False
    error_message: Optional[str] = None

    @property
    def duration(self) -> Optional[float]:
        if self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self.metrics: Dict[str, list[PerformanceMetrics]] = {}
        self._lock = asyncio.Lock()

    async def record_metric(self, metric: PerformanceMetrics):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        async with self._lock:
            if metric.operation not in self.metrics:
                self.metrics[metric.operation] = []
            self.metrics[metric.operation].append(metric)

    def get_operation_stats(self, operation: str, hours: int = 24) -> Dict:
        """è·å–æ“ä½œç»Ÿè®¡ä¿¡æ¯"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_metrics = [
            m for m in self.metrics.get(operation, [])
            if m.start_time >= cutoff_time
        ]

        if not recent_metrics:
            return {}

        successful = [m for m in recent_metrics if m.success]
        durations = [m.duration for m in successful if m.duration]

        return {
            "total_operations": len(recent_metrics),
            "successful_operations": len(successful),
            "success_rate": len(successful) / len(recent_metrics),
            "avg_duration": sum(durations) / len(durations) if durations else 0,
            "max_duration": max(durations) if durations else 0,
            "min_duration": min(durations) if durations else 0,
        }

# å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨
metrics = MetricsCollector()
```

**é¢„æœŸæ•ˆæœ**:
- âœ… ç»“æ„åŒ–æ—¥å¿—ä¾¿äºè‡ªåŠ¨åŒ–åˆ†æ
- âœ… æ€§èƒ½æŒ‡æ ‡å®æ—¶ç›‘æ§
- âœ… é—®é¢˜å¿«é€Ÿå®šä½å’Œæ ¹å› åˆ†æ

#### ğŸ“Š **å®æ–½æ—¶é—´**: 3-4å¤©
#### ğŸ§ª **æµ‹è¯•ç­–ç•¥**: æ—¥å¿—æ ¼å¼éªŒè¯ + æ€§èƒ½åŸºå‡†æµ‹è¯•
#### ğŸ“ˆ **ROI**: é«˜ - å¤§å¹…æå‡è¿ç»´æ•ˆç‡

---

### 3. **é…ç½®ç®¡ç†ä¼˜åŒ–**

#### ğŸ¯ **ç›®æ ‡**: æå‡é…ç½®çµæ´»æ€§å’Œç¯å¢ƒé€‚é…èƒ½åŠ›

#### ğŸ”§ **å®æ–½æ–¹æ¡ˆ**

**æ–‡ä»¶**: `src/config/settings.py` (æ–°å»º)

```python
from dataclasses import dataclass, field
from pathlib import Path
import os
from typing import Optional, List
from enum import Enum

class LogLevel(Enum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"

@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    ttl_hours: int = 24
    max_size_mb: int = 1000
    cleanup_interval_hours: int = 6
    enabled: bool = True

@dataclass
class APIConfig:
    """API é…ç½®"""
    timeout: int = 30
    max_retries: int = 3
    retry_delay: float = 1.0
    rate_limit_requests_per_minute: int = 60
    concurrent_requests: int = 10

@dataclass
class HKEXConfig:
    """HKEX Agent ä¸»é…ç½®"""

    # åŸºç¡€è·¯å¾„
    project_root: Path = field(default_factory=Path.cwd)

    # ç›®å½•é…ç½®
    pdf_cache_dir: Path = field(init=False)
    memories_dir: Path = field(init=False)
    md_output_dir: Path = field(init=False)
    logs_dir: Path = field(init=False)

    # ç»„ä»¶é…ç½®
    cache: CacheConfig = field(default_factory=CacheConfig)
    api: APIConfig = field(default_factory=APIConfig)

    # ç¯å¢ƒé…ç½®
    log_level: LogLevel = LogLevel.INFO
    environment: str = "development"

    # åŠŸèƒ½å¼€å…³
    enable_async_processing: bool = True
    enable_metrics: bool = True
    enable_hardware_acceleration: bool = False

    def __post_init__(self):
        """åˆå§‹åŒ–æ´¾ç”Ÿå­—æ®µ"""
        self.pdf_cache_dir = self.project_root / "pdf_cache"
        self.memories_dir = Path.home() / ".hkex-agent" / "memories"
        self.md_output_dir = self.project_root / "md"
        self.logs_dir = self.project_root / "logs"

        # åˆ›å»ºå¿…è¦ç›®å½•
        for directory in [self.pdf_cache_dir, self.memories_dir,
                         self.md_output_dir, self.logs_dir]:
            directory.mkdir(parents=True, exist_ok=True)

    @classmethod
    def from_env(cls) -> 'HKEXConfig':
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        config = cls()

        # è·¯å¾„é…ç½®
        if pdf_cache := os.getenv('PDF_CACHE_DIR'):
            config.pdf_cache_dir = Path(pdf_cache)
        if memories := os.getenv('MEMORIES_DIR'):
            config.memories_dir = Path(memories)
        if md_output := os.getenv('MD_OUTPUT_DIR'):
            config.md_output_dir = Path(md_output)

        # API é…ç½®
        config.api.timeout = int(os.getenv('API_TIMEOUT', str(config.api.timeout)))
        config.api.max_retries = int(os.getenv('MAX_RETRIES', str(config.api.max_retries)))

        # ç¼“å­˜é…ç½®
        config.cache.ttl_hours = int(os.getenv('CACHE_TTL_HOURS', str(config.cache.ttl_hours)))
        config.cache.max_size_mb = int(os.getenv('MAX_CACHE_SIZE_MB', str(config.cache.max_size_mb)))

        # ç¯å¢ƒé…ç½®
        config.log_level = LogLevel(os.getenv('LOG_LEVEL', config.log_level.value))
        config.environment = os.getenv('ENVIRONMENT', config.environment)

        # åŠŸèƒ½å¼€å…³
        config.enable_async_processing = os.getenv('ENABLE_ASYNC', 'true').lower() == 'true'
        config.enable_metrics = os.getenv('ENABLE_METRICS', 'true').lower() == 'true'

        return config

    def validate(self) -> List[str]:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        errors = []

        if self.api.timeout <= 0:
            errors.append("API timeout must be positive")
        if self.api.max_retries < 0:
            errors.append("Max retries must be non-negative")
        if self.cache.ttl_hours <= 0:
            errors.append("Cache TTL must be positive")
        if not self.project_root.exists():
            errors.append(f"Project root does not exist: {self.project_root}")

        return errors

# å…¨å±€é…ç½®å®ä¾‹
settings = HKEXConfig.from_env()
```

**æ–‡ä»¶**: `config/default.yaml` (æ–°å»º)

```yaml
# HKEX Agent é»˜è®¤é…ç½®
project:
  name: "HKEX Agent"
  version: "1.0.0"

cache:
  ttl_hours: 24
  max_size_mb: 1000
  cleanup_interval_hours: 6
  enabled: true

api:
  timeout: 30
  max_retries: 3
  retry_delay: 1.0
  rate_limit_requests_per_minute: 60
  concurrent_requests: 10

logging:
  level: "INFO"
  format: "json"
  file_output: true
  console_output: true

features:
  async_processing: true
  metrics: true
  hardware_acceleration: false

hkex:
  base_url: "https://www1.hkexnews.hk"
  user_agent: "HKEX-Agent/1.0"
  ssl_verify: true
```

**é¢„æœŸæ•ˆæœ**:
- âœ… ç¯å¢ƒé—´é…ç½®åˆ‡æ¢æ— ç¼
- âœ… é…ç½®éªŒè¯é˜²æ­¢è¿è¡Œæ—¶é”™è¯¯
- âœ… æ”¯æŒå¤šç§é…ç½®æºï¼ˆç¯å¢ƒå˜é‡ã€é…ç½®æ–‡ä»¶ï¼‰

#### ğŸ“Š **å®æ–½æ—¶é—´**: 2-3å¤©
#### ğŸ§ª **æµ‹è¯•ç­–ç•¥**: é…ç½®åŠ è½½éªŒè¯ + ç¯å¢ƒåˆ‡æ¢æµ‹è¯•
#### ğŸ“ˆ **ROI**: ä¸­ - æå‡éƒ¨ç½²çµæ´»æ€§

---

## ğŸ“Š **ä¸­ä¼˜å…ˆçº§ä¼˜åŒ–** (1-2å‘¨å†…æ‰§è¡Œ)

### 4. **å¼‚æ­¥å¤„ç†æ¶æ„**

#### ğŸ¯ **ç›®æ ‡**: æå‡ I/O å¯†é›†å‹æ“ä½œçš„æ€§èƒ½

#### ğŸ”§ **å®æ–½æ–¹æ¡ˆ**

**æ–‡ä»¶**: `src/services/async_pdf_service.py` (æ–°å»º)

```python
import asyncio
import aiofiles
import aiohttp
from pathlib import Path
from typing import Optional, List, Tuple
from dataclasses import dataclass
from datetime import datetime

@dataclass
class PDFDownloadTask:
    """PDF ä¸‹è½½ä»»åŠ¡"""
    stock_code: str
    date_time: str
    file_link: str
    title: str
    priority: int = 0  # ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜

class AsyncPDFService:
    """å¼‚æ­¥ PDF å¤„ç†æœåŠ¡"""

    def __init__(self, cache_dir: Path, max_concurrent: int = 5):
        self.cache_dir = cache_dir
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=60),
            connector=aiohttp.TCPConnector(limit=10)
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    def _get_cache_path(self, stock_code: str, date_time: str) -> Path:
        """ç”Ÿæˆç¼“å­˜è·¯å¾„"""
        # æ ¼å¼: pdf_cache/00673/2025/01/08/00673-20250108-123456.pdf
        date_obj = datetime.strptime(date_time, "%d/%m/%Y %H:%M")
        year_month = date_obj.strftime("%Y/%m")
        day = date_obj.strftime("%d")
        time_str = date_obj.strftime("%H%M%S")

        cache_dir = self.cache_dir / stock_code / year_month / day
        cache_dir.mkdir(parents=True, exist_ok=True)

        return cache_dir / f"{stock_code}-{date_obj.strftime('%Y%m%d')}-{time_str}.pdf"

    async def download_pdf_async(
        self,
        task: PDFDownloadTask
    ) -> Tuple[bool, Optional[Path]]:
        """å¼‚æ­¥ä¸‹è½½å•ä¸ª PDF"""
        async with self.semaphore:  # æ§åˆ¶å¹¶å‘æ•°
            pdf_path = self._get_cache_path(task.stock_code, task.date_time)

            # æ£€æŸ¥ç¼“å­˜
            if pdf_path.exists():
                return True, pdf_path

            if not self.session:
                raise RuntimeError("AsyncPDFService not properly initialized")

            try:
                async with self.session.get(task.file_link) as response:
                    response.raise_for_status()

                    async with aiofiles.open(pdf_path, 'wb') as f:
                        async for chunk in response.content.iter_chunked(8192):
                            await f.write(chunk)

                logger.info("PDFä¸‹è½½æˆåŠŸ",
                           stock_code=task.stock_code,
                           path=str(pdf_path))
                return True, pdf_path

            except Exception as e:
                logger.error("PDFä¸‹è½½å¤±è´¥",
                           stock_code=task.stock_code,
                           error=str(e))
                # æ¸…ç†éƒ¨åˆ†ä¸‹è½½çš„æ–‡ä»¶
                if pdf_path.exists():
                    pdf_path.unlink()
                return False, None

    async def download_batch(
        self,
        tasks: List[PDFDownloadTask]
    ) -> List[Tuple[PDFDownloadTask, bool, Optional[Path]]]:
        """æ‰¹é‡ä¸‹è½½ PDF"""
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_tasks = sorted(tasks, key=lambda t: t.priority)

        # å¹¶å‘ä¸‹è½½
        results = await asyncio.gather(
            *[self.download_pdf_async(task) for task in sorted_tasks],
            return_exceptions=True
        )

        # ç»„è£…ç»“æœ
        return list(zip(sorted_tasks, results))

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""

    def __init__(self, pdf_service: AsyncPDFService):
        self.pdf_service = pdf_service
        self.batch_size = 10

    async def process_announcements(
        self,
        announcements: List[Dict]
    ) -> List[Dict]:
        """æ‰¹é‡å¤„ç†å…¬å‘Š"""
        # åˆ›å»ºä¸‹è½½ä»»åŠ¡
        download_tasks = []
        for announcement in announcements:
            task = PDFDownloadTask(
                stock_code=announcement['STOCK_CODE'],
                date_time=announcement['DATE_TIME'],
                file_link=announcement['FILE_LINK'],
                title=announcement['TITLE']
            )
            download_tasks.append(task)

        # åˆ†æ‰¹å¤„ç†
        all_results = []
        for i in range(0, len(download_tasks), self.batch_size):
            batch = download_tasks[i:i + self.batch_size]
            batch_results = await self.pdf_service.download_batch(batch)
            all_results.extend(batch_results)

            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            if i + self.batch_size < len(download_tasks):
                await asyncio.sleep(1)

        return all_results

# ä½¿ç”¨ç¤ºä¾‹
async def process_hkex_announcements(announcements: List[Dict]) -> List[Dict]:
    """å¤„ç†æ¸¯äº¤æ‰€å…¬å‘Šçš„å¼‚æ­¥å‡½æ•°"""
    async with AsyncPDFService(Path("pdf_cache")) as pdf_service:
        processor = BatchProcessor(pdf_service)
        results = await processor.process_announcements(announcements)
        return results
```

**é¢„æœŸæ•ˆæœ**:
- âœ… å¹¶å‘ä¸‹è½½æ€§èƒ½æå‡ 5-10 å€
- âœ… æ›´å¥½çš„èµ„æºåˆ©ç”¨ç‡
- âœ… æ”¯æŒå¤§è§„æ¨¡æ‰¹é‡å¤„ç†

#### ğŸ“Š **å®æ–½æ—¶é—´**: 5-7å¤©
#### ğŸ§ª **æµ‹è¯•ç­–ç•¥**: å¹¶å‘æ€§èƒ½æµ‹è¯• + èµ„æºä½¿ç”¨ç›‘æ§
#### ğŸ“ˆ **ROI**: ä¸­ - æ˜¾è‘—æå‡å¤§æ‰¹é‡å¤„ç†æ€§èƒ½

---

### 5. **æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ**

#### ğŸ¯ **ç›®æ ‡**: ä¼˜åŒ–ç¼“å­˜ç­–ç•¥å’Œå­˜å‚¨æ•ˆç‡

#### ğŸ”§ **å®æ–½æ–¹æ¡ˆ**

**æ–‡ä»¶**: `src/cache/smart_cache.py` (æ–°å»º)

```python
import asyncio
import json
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, asdict
import shutil
import gzip

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str
    filename: str
    content_type: str
    size_bytes: int
    cached_at: datetime
    expires_at: Optional[datetime] = None
    access_count: int = 0
    last_accessed: Optional[datetime] = None
    metadata: Dict[str, Any] = None

    def is_expired(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        if self.expires_at is None:
            return False
        return datetime.now() > self.expires_at

    def should_evict(self, max_age_days: int = 30) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è¢«é©±é€"""
        age = datetime.now() - self.cached_at
        return age.days > max_age_days

class SmartCacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: Path, max_size_gb: float = 5.0):
        self.cache_dir = cache_dir
        self.max_size_bytes = int(max_size_gb * 1024 * 1024 * 1024)
        self.metadata_file = cache_dir / 'cache_metadata.json'
        self.lock = asyncio.Lock()

        cache_dir.mkdir(parents=True, exist_ok=True)
        self._metadata: Dict[str, CacheEntry] = {}
        asyncio.create_task(self._load_metadata())

    def _generate_key(self, prefix: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{prefix}:{json.dumps(kwargs, sort_keys=True)}"
        return hashlib.sha256(key_data.encode()).hexdigest()[:16]

    async def _load_metadata(self):
        """åŠ è½½ç¼“å­˜å…ƒæ•°æ®"""
        async with self.lock:
            if self.metadata_file.exists():
                try:
                    async with aiofiles.open(self.metadata_file, 'r') as f:
                        content = await f.read()
                        data = json.loads(content)

                        for key, entry_data in data.items():
                            entry_data['cached_at'] = datetime.fromisoformat(entry_data['cached_at'])
                            if entry_data.get('expires_at'):
                                entry_data['expires_at'] = datetime.fromisoformat(entry_data['expires_at'])
                            if entry_data.get('last_accessed'):
                                entry_data['last_accessed'] = datetime.fromisoformat(entry_data['last_accessed'])

                            self._metadata[key] = CacheEntry(**entry_data)
                except Exception as e:
                    logger.error("åŠ è½½ç¼“å­˜å…ƒæ•°æ®å¤±è´¥", error=str(e))
                    self._metadata = {}

    async def _save_metadata(self):
        """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®"""
        async with self.lock:
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            serializable = {}
            for key, entry in self._metadata.items():
                data = asdict(entry)
                data['cached_at'] = entry.cached_at.isoformat()
                if entry.expires_at:
                    data['expires_at'] = entry.expires_at.isoformat()
                if entry.last_accessed:
                    data['last_accessed'] = entry.last_accessed.isoformat()
                serializable[key] = data

            # åŸå­å†™å…¥
            temp_file = self.metadata_file.with_suffix('.tmp')
            async with aiofiles.open(temp_file, 'w') as f:
                await f.write(json.dumps(serializable, indent=2))
            temp_file.rename(self.metadata_file)

    async def get(self, key: str) -> Optional[Path]:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        async with self.lock:
            if key not in self._metadata:
                return None

            entry = self._metadata[key]

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            cache_file = self.cache_dir / entry.filename
            if not cache_file.exists():
                # æ¸…ç†æ— æ•ˆæ¡ç›®
                del self._metadata[key]
                await self._save_metadata()
                return None

            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if entry.is_expired():
                await self._remove_entry(key)
                return None

            # æ›´æ–°è®¿é—®ä¿¡æ¯
            entry.access_count += 1
            entry.last_accessed = datetime.now()
            asyncio.create_task(self._save_metadata())

            return cache_file

    async def put(
        self,
        key: str,
        content: bytes,
        content_type: str = "application/octet-stream",
        ttl_hours: Optional[int] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Path:
        """å­˜å‚¨å†…å®¹åˆ°ç¼“å­˜"""
        async with self.lock:
            # å¦‚æœå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if key in self._metadata:
                await self._remove_entry(key)

            # ç”Ÿæˆæ–‡ä»¶å
            filename = f"{key}.{self._get_extension(content_type)}"
            if content_type == "application/pdf":
                # PDF æ–‡ä»¶å‹ç¼©å­˜å‚¨
                filename += ".gz"
                compressed_content = gzip.compress(content)
                content_bytes = len(compressed_content)
            else:
                compressed_content = content
                content_bytes = len(content)

            # æ£€æŸ¥ç©ºé—´é™åˆ¶
            await self._ensure_space(content_bytes)

            # å†™å…¥æ–‡ä»¶
            cache_file = self.cache_dir / filename
            async with aiofiles.open(cache_file, 'wb') as f:
                await f.write(compressed_content)

            # åˆ›å»ºå…ƒæ•°æ®æ¡ç›®
            expires_at = None
            if ttl_hours:
                expires_at = datetime.now() + timedelta(hours=ttl_hours)

            entry = CacheEntry(
                key=key,
                filename=filename,
                content_type=content_type,
                size_bytes=content_bytes,
                cached_at=datetime.now(),
                expires_at=expires_at,
                metadata=metadata or {}
            )

            self._metadata[key] = entry
            await self._save_metadata()

            return cache_file

    async def _remove_entry(self, key: str):
        """åˆ é™¤ç¼“å­˜æ¡ç›®"""
        if key in self._metadata:
            entry = self._metadata[key]
            cache_file = self.cache_dir / entry.filename
            if cache_file.exists():
                cache_file.unlink()
            del self._metadata[key]

    async def _ensure_space(self, required_bytes: int):
        """ç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´"""
        current_size = sum(entry.size_bytes for entry in self._metadata.values())

        if current_size + required_bytes > self.max_size_bytes:
            # æŒ‰LRUç­–ç•¥æ¸…ç†ç©ºé—´
            entries_by_access = sorted(
                self._metadata.values(),
                key=lambda e: (e.last_accessed or e.cached_at)
            )

            freed_bytes = 0
            for entry in entries_by_access:
                await self._remove_entry(entry.key)
                freed_bytes += entry.size_bytes

                if current_size - freed_bytes + required_bytes <= self.max_size_bytes:
                    break

    async def cleanup_expired(self) -> int:
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        async with self.lock:
            expired_keys = [
                key for key, entry in self._metadata.items()
                if entry.is_expired() or entry.should_evict()
            ]

            for key in expired_keys:
                await self._remove_entry(key)

            if expired_keys:
                await self._save_metadata()

            return len(expired_keys)

    def _get_extension(self, content_type: str) -> str:
        """æ ¹æ®å†…å®¹ç±»å‹è·å–æ–‡ä»¶æ‰©å±•å"""
        extensions = {
            "application/pdf": "pdf",
            "text/plain": "txt",
            "application/json": "json",
            "text/markdown": "md"
        }
        return extensions.get(content_type, "bin")

    async def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        async with self.lock:
            total_entries = len(self._metadata)
            total_size = sum(entry.size_bytes for entry in self._metadata.values())

            expired_count = sum(1 for entry in self._metadata.values() if entry.is_expired())

            # è®¿é—®é¢‘ç‡ç»Ÿè®¡
            access_counts = [entry.access_count for entry in self._metadata.values()]
            avg_access = sum(access_counts) / len(access_counts) if access_counts else 0

            return {
                "total_entries": total_entries,
                "total_size_bytes": total_size,
                "total_size_mb": round(total_size / (1024 * 1024), 2),
                "expired_entries": expired_count,
                "cache_hit_rate": 0.0,  # éœ€è¦é¢å¤–çš„å‘½ä¸­ç‡ç»Ÿè®¡
                "avg_access_count": round(avg_access, 2),
                "utilization_percent": round((total_size / self.max_size_bytes) * 100, 2)
            }

# å…¨å±€ç¼“å­˜ç®¡ç†å™¨
cache_manager = SmartCacheManager(Path("cache"))
```

**é¢„æœŸæ•ˆæœ**:
- âœ… æ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼Œè‡ªåŠ¨æ¸…ç†è¿‡æœŸå†…å®¹
- âœ… å­˜å‚¨ç©ºé—´ä¼˜åŒ–ï¼Œæ”¯æŒå‹ç¼©
- âœ… LRU é©±é€ç­–ç•¥ï¼Œæå‡ç¼“å­˜å‘½ä¸­ç‡

#### ğŸ“Š **å®æ–½æ—¶é—´**: 4-5å¤©
#### ğŸ§ª **æµ‹è¯•ç­–ç•¥**: ç¼“å­˜æ€§èƒ½æµ‹è¯• + å¹¶å‘è¯»å†™æµ‹è¯•
#### ğŸ“ˆ **ROI**: ä¸­ - ä¼˜åŒ–å­˜å‚¨æ•ˆç‡ï¼Œæå‡ç¼“å­˜å‘½ä¸­ç‡

---

## ğŸ”® **ä½ä¼˜å…ˆçº§ä¼˜åŒ–** (é•¿æœŸè§„åˆ’)

### 6. **äº‹ä»¶é©±åŠ¨æ¶æ„é‡æ„**

#### ğŸ¯ **ç›®æ ‡**: æå‡ç³»ç»Ÿè§£è€¦å’Œæ‰©å±•æ€§

#### ğŸ“Š **å®æ–½æ—¶é—´**: 2-3å‘¨
#### ğŸ“ˆ **ROI**: é•¿æœŸä»·å€¼ - ä¸ºå¾®æœåŠ¡åŒ–å¥ å®šåŸºç¡€

### 7. **å¾®æœåŠ¡æ¶æ„æ¼”è¿›**

#### ğŸ¯ **ç›®æ ‡**: æ”¯æŒé«˜å¹¶å‘å’Œç‹¬ç«‹éƒ¨ç½²

#### ğŸ“Š **å®æ–½æ—¶é—´**: 1-2ä¸ªæœˆ
#### ğŸ“ˆ **ROI**: æˆ˜ç•¥ä»·å€¼ - æ”¯æŒè§„æ¨¡åŒ–æ‰©å±•

### 8. **å®¹å™¨åŒ–ä¸äº‘åŸç”Ÿéƒ¨ç½²**

#### ğŸ¯ **ç›®æ ‡**: æå‡éƒ¨ç½²æ•ˆç‡å’Œå¯ç§»æ¤æ€§

#### ğŸ“Š **å®æ–½æ—¶é—´**: 1-2å‘¨
#### ğŸ“ˆ **ROI**: è¿ç»´ä»·å€¼ - ç®€åŒ–éƒ¨ç½²å’Œè¿ç»´

---

## ğŸ“ˆ **å®æ–½è·¯çº¿å›¾**

### **ç¬¬1é˜¶æ®µ (1-2å‘¨)** - ç¨³å®šæ€§æå‡
- âœ… é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶
- âœ… ç»“æ„åŒ–æ—¥å¿—ä¸ç›‘æ§
- âœ… é…ç½®ç®¡ç†ä¼˜åŒ–

### **ç¬¬2é˜¶æ®µ (3-4å‘¨)** - æ€§èƒ½ä¼˜åŒ–
- âœ… å¼‚æ­¥å¤„ç†æ¶æ„
- âœ… æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ

### **ç¬¬3é˜¶æ®µ (2-3ä¸ªæœˆ)** - æ¶æ„æ¼”è¿›
- âœ… äº‹ä»¶é©±åŠ¨æ¶æ„
- âœ… å¾®æœåŠ¡æ‹†åˆ†è§„åˆ’
- âœ… å®¹å™¨åŒ–éƒ¨ç½²

---

## ğŸ¯ **æˆåŠŸæŒ‡æ ‡**

### **æŠ€æœ¯æŒ‡æ ‡**
- ç³»ç»Ÿå¯ç”¨æ€§: 99.9% â†’ 99.95%
- APIå“åº”æ—¶é—´: P95 < 2s â†’ P95 < 500ms
- ç¼“å­˜å‘½ä¸­ç‡: 60% â†’ 85%
- é”™è¯¯ç‡: < 1% â†’ < 0.1%

### **ä¸šåŠ¡æŒ‡æ ‡**
- å¤„ç†èƒ½åŠ›: 100 ä»½/å¤© â†’ 1000 ä»½/å¤©
- ç”¨æˆ·æ»¡æ„åº¦: 4.0/5 â†’ 4.5/5
- è¿ç»´æˆæœ¬: é™ä½ 40%

---

## ğŸ“‹ **é£é™©è¯„ä¼°**

### **é«˜é£é™©**
- å¼‚æ­¥é‡æ„å¯èƒ½å¼•å…¥æ–°bug
- ç¼“å­˜ç³»ç»Ÿå¤æ‚åº¦å¢åŠ 

### **ä¸­é£é™©**
- é…ç½®å˜æ›´å½±å“ç°æœ‰éƒ¨ç½²
- æ—¥å¿—ç³»ç»Ÿæ€§èƒ½å½±å“

### **ç¼“è§£ç­–ç•¥**
- æ¸è¿›å¼é‡æ„ï¼Œä¿æŒå‘åå…¼å®¹
- å®Œå–„çš„æµ‹è¯•è¦†ç›–
- ç°åº¦å‘å¸ƒç­–ç•¥

---

## ğŸ’¡ **æ€»ç»“**

é€šè¿‡è¿™8ä¸ªä¼˜åŒ–å»ºè®®çš„å®æ–½ï¼ŒHKEXé¡¹ç›®å°†å®ç°ï¼š

1. **ç¨³å®šæ€§æ˜¾è‘—æå‡** - é€šè¿‡é”™è¯¯å¤„ç†å’Œç›‘æ§
2. **æ€§èƒ½å¤§å¹…ä¼˜åŒ–** - é€šè¿‡å¼‚æ­¥å¤„ç†å’Œæ™ºèƒ½ç¼“å­˜
3. **å¯ç»´æŠ¤æ€§å¢å¼º** - é€šè¿‡é…ç½®ç®¡ç†å’Œç»“æ„åŒ–æ—¥å¿—
4. **æ‰©å±•æ€§æå‡** - ä¸ºå¾®æœåŠ¡åŒ–å’Œäº‘åŸç”Ÿå¥ å®šåŸºç¡€

**å»ºè®®ä¼˜å…ˆå®æ–½é«˜ä¼˜å…ˆçº§ä¼˜åŒ–é¡¹ï¼Œé¢„è®¡2-3å‘¨å†…å³å¯çœ‹åˆ°æ˜¾è‘—æ•ˆæœã€‚**

---

*æœ¬ä¼˜åŒ–å»ºè®®åŸºäºå®é™…ä»£ç åˆ†æï¼Œå»ºè®®ç»“åˆå…·ä½“ä¸šåŠ¡éœ€æ±‚å’Œèµ„æºæƒ…å†µè°ƒæ•´å®æ–½ä¼˜å…ˆçº§ã€‚*